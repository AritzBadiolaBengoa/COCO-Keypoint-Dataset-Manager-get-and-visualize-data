{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hi! This application/set of code has been created in order to help researchers or students interested in\n",
    "keypoint detection or Human Pose Estimation to obtain and visualize the data from COCO dataset in an easy\n",
    "and intuitive way, just using few lines of code.\n",
    "\n",
    "@AritzBadiolaBengoa\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from ipynb.fs.full.defs import * #importing defs for download_unzip function\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpus: 16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "COCO DATASET DOWNLOADING AND UNZIPPING\n",
    "\n",
    "'''\n",
    "\n",
    "# This code downloads the coco dataset from Amazon S3 in parallel.\n",
    "\n",
    "files = ['val2017.zip', 'annotations_trainval2017.zip', 'train2017.zip']\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(\"Number of cpus: \" + str(num_cpus))\n",
    "with multiprocessing.Pool(num_cpus) as p:\n",
    "    p.map(download_unzip, files)\n",
    "    \n",
    "print(\"COCO downloaded and unzipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FIXED PARAMETERS\n",
    "'''\n",
    "\n",
    "#DIMENSIONS OF RESIZED IMAGE (INPUT FOR THE MODEL)\n",
    "IMG_WIDTH = 192\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "#PARAMETERS OF THE KEYPOINTS OF THE DATASET\n",
    "N_DIM = 3 #x,y,visibility\n",
    "N_KEYPOINTS = 17 #it is set later in the code just in case\n",
    "K_NAMES = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle'] #it is set later in the code just in case\n",
    "\n",
    "#PATHS\n",
    "TRAIN_ANNOT_PATH = 'coco_dataset/annotations/person_keypoints_train2017.json' #annotations\n",
    "VAL_ANNOT_PATH = 'coco_dataset/annotations/person_keypoints_val2017.json' #annotations\n",
    "PATH_TRAIN = 'coco_dataset/train2017' #images\n",
    "PATH_VAL = 'coco_dataset/val2017' #images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING JSON FILES\n",
    "\n",
    "with open(TRAIN_ANNOT_PATH) as f:\n",
    "  train_coco = json.load(f) # load annotations for training set \n",
    "\n",
    "with open(VAL_ANNOT_PATH) as f:\n",
    "  val_coco = json.load(f) # load annotations for validation set \n",
    "\n",
    "N_KEYPOINTS = len(train_coco['categories'][0]['keypoints']) #17\n",
    "K_NAMES = train_coco['categories'][0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING ATTRIBUTES OF A SAMPLE FROM THE DATASET\n",
    "\n",
    "#function to get the attributes of the given n-th sample from the dataset\n",
    "def get_sample_att(torval,sample_n): #torval -> 0 if training and 1 if validation dataset\n",
    "    sample = val_coco['annotations'][sample_n] if torval else train_coco['annotations'][sample_n]\n",
    "    img_id = str(sample['image_id'])\n",
    "    k_list = sample['keypoints']\n",
    "    bbox = sample['bbox']\n",
    "    is_crowd = sample['iscrowd']\n",
    "\n",
    "    #the keypoints in each sample are indicated in a list as: x1, y1, v1, x2, y2, v2, x3... and there are \n",
    "    #17 keypoints, so, we need it in a bidimensional array of 17x3\n",
    "\n",
    "    k_array = np.asarray(k_list)\n",
    "    k_array3d = np.reshape(k_array,(N_KEYPOINTS,N_DIM))\n",
    "    keypoints = k_array3d[:,:2]\n",
    "    k_vis = k_array3d[:,2]\n",
    "    \n",
    "    return img_id,bbox,is_crowd,keypoints,k_vis\n",
    "\n",
    "#example of getting the attributes of the 0th sample of the dataset\n",
    "#img_id,bbox,is_crowd,keypoints,k_vis = get_sample_att(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(torval, img_id):\n",
    "    #load image\n",
    "    #the image names are 12 digits and the first gaps are filled with 0's\n",
    "    img_name = '000000000000'\n",
    "    img_name = img_name[0:len(img_name)-len(img_id)] + img_id + '.jpg'\n",
    "    path = PATH_VAL if torval else PATH_TRAIN\n",
    "    og_img = Image.open(path + '/' + img_name)\n",
    "    return og_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some of the samples of the dataset have keypoints drawn out of the bounding box and I propose the following solution\n",
    "#making the bounding box as big as necessary, if it is possible, to include the keypoints out of the area, as this part of the image can be interesting for the training or validation of the model\n",
    "def check_keypoints_in_bbox(bbox,keypoints,k_vis,og_img):\n",
    "    #attributes of the bounding box and original image\n",
    "    bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "    img_w, img_h = og_img.size\n",
    "    \n",
    "    #calculate min and max values of x and y positions of the keypoints\n",
    "    x_min = 99999\n",
    "    x_max = -1\n",
    "    y_min = 99999\n",
    "    y_max = -1\n",
    "    for i in range(len(keypoints)):\n",
    "        if k_vis[i] > 0:\n",
    "            x_temp = keypoints[i][0]\n",
    "            y_temp = keypoints[i][1]\n",
    "            if x_temp < x_min: x_min = x_temp\n",
    "            if x_temp > x_max: x_max = x_temp\n",
    "            if y_temp < y_min: y_min = y_temp\n",
    "            if y_temp > y_max: y_max = y_temp\n",
    "    x_min = x_min-10\n",
    "    x_max = x_max+10\n",
    "    y_min = y_min-10\n",
    "    y_max = y_max+10\n",
    "        \n",
    "    if x_min < bbox_x:\n",
    "        if x_min < 0: x_min = 0\n",
    "    else:\n",
    "        x_min = bbox_x\n",
    "    \n",
    "    if x_max > bbox_x+bbox_w:\n",
    "        if x_max > img_w: x_max = img_w\n",
    "    else:\n",
    "        x_max = bbox_x+bbox_w\n",
    "        \n",
    "    if y_min < bbox_y:\n",
    "        if y_min < 0: y_min = 0\n",
    "    else:\n",
    "        y_min = bbox_y\n",
    "        \n",
    "    if y_max > bbox_y+bbox_h:\n",
    "        if y_max > img_h: y_max = img_h\n",
    "    else:\n",
    "        y_max = bbox_y+bbox_h\n",
    "        \n",
    "    bbox = [x_min,y_min,x_max-x_min,y_max-y_min]\n",
    "    return bbox\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROP IMAGE IN THE BOUNDING BOX AND RESIZE TO INPUT IMAGE SIZE\n",
    "\n",
    "def crop_resize_img(torval,og_img,bbox): #torval -> 0 if training and 1 if validation dataset\n",
    "    #attributes of the bounding box\n",
    "    bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "    \n",
    "    #resize image part of the bounding box\n",
    "    res_img = og_img.resize((IMG_WIDTH, IMG_HEIGHT), box=(bbox_x,bbox_y,bbox_x+bbox_w,bbox_y+bbox_h))\n",
    "    \n",
    "    return res_img\n",
    "\n",
    "#og_img,res_img = crop_resize_img(0,img_id,bbox) #load original and resized images of the first sample from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESCALE KEYPOINTS TO MATCH RESIZED IMAGE\n",
    "#crop -> rest to the keypoint position the bounding box position as the keypoint should start from there\n",
    "#rescale -> as the image has been resized, the position of the keypoint must be adapted taking into account the scale from the original image to the resized one\n",
    "def rescale_keypoints(keypoints,bbox):\n",
    "    bbox_x,bbox_y,bbox_w,bbox_h = bbox\n",
    "    box_start_pos = np.asarray([bbox_x,bbox_y])\n",
    "    box_size = np.asarray([bbox_w,bbox_h])\n",
    "    res_size = np.asarray([IMG_WIDTH,IMG_HEIGHT])\n",
    "    keypoints = np.round((keypoints-box_start_pos)*(res_size/box_size)).astype(int)\n",
    "\n",
    "    keypoints[keypoints<0] = 0 #if the original value was 0, then it will be converted to negative, so it should be reconverted to 0\n",
    "    \n",
    "    return keypoints\n",
    "    \n",
    "#keypoints = rescale_keypoints(keypoints,bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY RESIZED IMAGE AND KEYPOINTS\n",
    "\n",
    "#draw keypoints in the resized image\n",
    "def plot_keypoints(draw_non_visible,keypoints,k_vis,ax): #draw_non_visible -> option to draw (1) or not (0) keypoints that are in the dataset but not visible in the image (occlusions)\n",
    "    for j in range(len(K_NAMES)):\n",
    "        n = K_NAMES[j]\n",
    "        x = keypoints[j,0]\n",
    "        y = keypoints[j,1]\n",
    "\n",
    "        if k_vis[j] > 0 and (draw_non_visible or (draw_non_visible == 0 and k_vis[j] == 2)):\n",
    "          ax.scatter(x, y, 250)\n",
    "          ax.text(x+5, y+5, n, fontsize=18, bbox=dict(facecolor='r', alpha=0.5), color='w')\n",
    "\n",
    "def display_images_keypoints(draw_non_visible,og_img,bbox,res_img,keypoints,k_vis):\n",
    "    #display original image with bounding box and resized image with keypoints\n",
    "    bbox_x,bbox_y,bbox_w,bbox_h = bbox\n",
    "    fig = plt.figure(1,figsize=(20,20))\n",
    "    ax1 = fig.add_subplot(121)  # left side\n",
    "    ax1.imshow(og_img)\n",
    "    ax1.add_patch(Rectangle((bbox_x, bbox_y), bbox_w, bbox_h, alpha=0.3, facecolor=\"blue\", edgecolor=\"red\", hatch='x'))\n",
    "\n",
    "    ax2 = fig.add_subplot(122)  # right side\n",
    "    ax2.imshow(res_img)\n",
    "    ax2 = plot_keypoints(draw_non_visible,keypoints,k_vis,ax2)\n",
    "    plt.show()\n",
    "    \n",
    "#display_images_keypoints(DRAW_NON_VISIBLE,og_img,bbox,res_img,keypoints,k_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LOAD THE SELECTED SAMPLE BY SPECIFYING THE NUMBER FROM THE TRAINING/VALIDATION DATASET\n",
    "'''\n",
    "#OPTION PARAMETERS FOR THE SAMPLE LOADING\n",
    "SAMPLE_N = 0 #sample number from the dataset\n",
    "TRAIN_VAL = 0 # 0 -> training dataset, 1 -> validation dataset\n",
    "DRAW_NON_VISIBLE = 1 # 0 -> dont draw non visible keypoints, 1 -> draw non visible keypoints\n",
    "\n",
    "img_id,bbox,is_crowd,keypoints,k_vis = get_sample_att(TRAIN_VAL,SAMPLE_N)\n",
    "print('Is crowd: ' + str(is_crowd))\n",
    "og_img = get_img(TRAIN_VAL, img_id)\n",
    "bbox = check_keypoints_in_bbox(bbox,keypoints,k_vis,og_img)\n",
    "print('Bounding box: ' + str(bbox))\n",
    "res_img = crop_resize_img(TRAIN_VAL,og_img,bbox)\n",
    "keypoints = rescale_keypoints(keypoints,bbox)\n",
    "print('KEYPOINTS POSITIONS: ')\n",
    "for i in range(len(keypoints)):\n",
    "    if k_vis[i] > 0:\n",
    "        print('- ' + str(K_NAMES[i]) + ': ' + str(keypoints[i]) + ' (visibility: ' + str(k_vis[i]) + ')')\n",
    "    else:\n",
    "        print('- ' + str(K_NAMES[i]) + ': not specified')\n",
    "print('*visibility: 1 (non visible but specified), 2 (visible)*')\n",
    "display_images_keypoints(DRAW_NON_VISIBLE,og_img,bbox,res_img,keypoints,k_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LOAD ALL SAMPLES FROM THE SELECTED DATASET (TRAINING/VALIDATION)\n",
    "'''\n",
    "#OPTION PARAMETERS FOR THE SAMPLE LOADING\n",
    "TRAIN_VAL = 1\n",
    "DRAW_NON_VISIBLE = 1\n",
    "for i in range(len(val_coco['annotations'])):\n",
    "    img_id,bbox,is_crowd,keypoints,k_vis = get_sample_att(TRAIN_VAL,i)\n",
    "    og_img = get_img(TRAIN_VAL, img_id)\n",
    "    bbox = check_keypoints_in_bbox(bbox,keypoints,k_vis,og_img)\n",
    "    print('Bounding box: ' + str(bbox))\n",
    "    res_img = crop_resize_img(TRAIN_VAL,og_img,bbox)\n",
    "    keypoints = rescale_keypoints(keypoints,bbox)\n",
    "    print('KEYPOINTS POSITIONS: ')\n",
    "    for i in range(len(keypoints)):\n",
    "        if k_vis[i] > 0:\n",
    "            print('- ' + str(K_NAMES[i]) + ': ' + str(keypoints[i]) + ' (visibility: ' + str(k_vis[i]) + ')')\n",
    "        else:\n",
    "            print('- ' + str(K_NAMES[i]) + ': not specified')\n",
    "    print('*visibility: 1 (non visible but specified), 2 (visible)*')\n",
    "    display_images_keypoints(DRAW_NON_VISIBLE,og_img,bbox,res_img,keypoints,k_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
